{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
    "\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=1, size=(1, 256, 256)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "        im_chan: the number of channels of the output image, a scalar\n",
    "              (MNIST is black-and-white, so 1 channel is your default)\n",
    "    hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, im_chan=3, hidden_dim=16):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self.make_disc_block(im_chan, hidden_dim, stride=4),\n",
    "            self.make_disc_block(hidden_dim, hidden_dim * 2, stride=3),\n",
    "            self.make_disc_block(hidden_dim * 2, hidden_dim * 2, stride=3),\n",
    "            self.make_disc_block(hidden_dim * 2, 1, kernel_size=5, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a discriminator block of DCGAN, \n",
    "        corresponding to a convolution, a batchnorm (except for in the last layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        #     Steps:\n",
    "        #       1) Add a convolutional layer using the given parameters.\n",
    "        #       2) Do a batchnorm, except for the last layer.\n",
    "        #       3) Follow each batchnorm with a LeakyReLU activation with slope 0.2.\n",
    "        \n",
    "        # Build the neural block\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                #### START CODE HERE #### #\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "                #### END CODE HERE ####\n",
    "            )\n",
    "        else: # Final Layer\n",
    "            return nn.Sequential(\n",
    "                #### START CODE HERE #### #\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n",
    "                #### END CODE HERE ####\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the discriminator: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with dimension (im_dim)\n",
    "        '''\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "z_dim = 64\n",
    "display_step = 117\n",
    "batch_size = 30\n",
    "# A learning rate of 0.0002 works well on DCGAN\n",
    "lr = 0.0002\n",
    "\n",
    "# These parameters control the optimizer's momentum, which you can read more about here:\n",
    "# https://distill.pub/2017/momentum/ but you donâ€™t need to worry about it for this course!\n",
    "beta_1 = 0.5 \n",
    "beta_2 = 0.999\n",
    "device = 'cuda'\n",
    "\n",
    "# You can tranform the image values to be between -1 and 1 (the range of the tanh activation)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(root=\"disc-data/ukiyoe\", \n",
    "                                            transform=transform)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "disc = Discriminator().to(device) \n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "\n",
    "# You initialize the weights to the normal distribution\n",
    "# with mean 0 and standard deviation 0.02\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(disc_steps, disc_losses, t='van-gogh'):\n",
    "    plt.figure()\n",
    "    plt.plot(disc_steps, disc_losses)\n",
    "    if t == 'van-gogh':\n",
    "        plt.title('Discriminator loss for Van Gogh paintings')\n",
    "    elif t == 'ukiyoe':\n",
    "        plt.title('Discriminator loss for Ukiyoe paintings')\n",
    "    else:\n",
    "        plt.title('Discriminator loss for Monet paintings')\n",
    "        \n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Mean loss')\n",
    "    plt.savefig('disc-results/plots/' + t + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 100\n",
    "cur_step = 0\n",
    "mean_discriminator_loss = 0\n",
    "disc_losses = []\n",
    "disc_steps = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches\n",
    "    for img, label in tqdm(dataloader):\n",
    "        cur_batch_size = len(img)\n",
    "        img = img.to(device)\n",
    "\n",
    "        ## Update discriminator ##\n",
    "        disc_opt.zero_grad()\n",
    "        \n",
    "        disc_pred = disc(img)\n",
    "        disc_pred = disc_pred[:,0]\n",
    "        label = label.type_as(disc_pred)\n",
    "        disc_loss = criterion(disc_pred, label)\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        # Update optimizer\n",
    "        disc_opt.step()\n",
    "        \n",
    "        # Keep track of the average discriminator loss\n",
    "        mean_discriminator_loss += disc_loss.item() / display_step\n",
    "        \n",
    "        ## Visualization code ##\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            print(\"Step \" + str(cur_step) + ': ' + str(mean_discriminator_loss))\n",
    "            disc_losses.append(mean_discriminator_loss)\n",
    "            disc_steps.append(cur_step)\n",
    "            plot_loss(disc_steps, disc_losses, t='ukiyoe')\n",
    "            mean_discriminator_loss = 0\n",
    "            torch.save(disc.state_dict(), 'disc-results/models/ukiyoe_disc_latest.pth')\n",
    "            \n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
